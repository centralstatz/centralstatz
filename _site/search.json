[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow do you assess the proportional-odds assumption? Directly.\n\n\n\n\n\n\nRegression Modeling\n\n\n\nA simple visual check.\n\n\n\n\n\nJul 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA simple example why statistical significance is insufficient for action\n\n\n\n\n\n\nStatistical Significance\n\n\nDecision Making\n\n\n\nIt ignores the basic question: “How much?”\n\n\n\n\n\nJun 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\n5 ways to help ensure success of a statistical project\n\n\n\n\n\n\nProject Management\n\n\n\nHow can we increase the likelihood that things will go as expected?\n\n\n\n\n\nMay 16, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "What we do",
    "section": "",
    "text": "Statistical and data science consulting in Central Wisconsin\n\n\n\n\nWhat we do\n\n\n  \n    \n      \n          statistical design, modeling and inference\n      \n    \n    \n      \n        Statistics is all about sampling data to learn something about a broader population. We'll help you determine the proper design and methodology to get the answers to questions you need to drive insight, decision and action.\n      \n    \n\n  \n    \n      \n        application and code development\n      \n    \n    \n      \n        \n        \n        R is a widely used open-source (free!) programming language, and we have over a decade of experience using it for complex data science tasks. It's our tool of choice for statistical computing and we can help you leverage it in diverse ways at relatively low cost. Here are some of the things we can do:\n        \n        \n          Perform complex data manipulation and visualization using the tidyverse\n          Build and assess statistical and predictive models\n          Design, develop and deploy custom Shiny web applications\n          Produce streamlined or automated analytical reporting workflows with R Markdown and Quarto\n          Develop and maintain R Packages for efficient use of shareable code\n        \n        We use version control for all of our code development so you can ensure transparency.\n        \n        Need help with other data science software/tools such as Python, SQL, or something else? Let us know!\n      \n  \n\n  \n    \n      \n        technical writing and manuscript review\n      \n    \n    \n      \n        Submitting research for publication? Trying to win grant funding for a study? We can review your documents for statistical soundness, or help you write it altogether. \n      \n  \n    \n  \n    \n      \n        predictive analytics and machine learning\n      \n    \n    \n      \n        Building predictive models is one thing, realizing tangible impact with them is another (Hint: it's not about more complex algorithms). We'll help you develop practical modeling solutions that are tailored to your decison-making processes and informed by your workflow, technological and resource constraints.\n      \n  \n\n  \n    \n      \n        training, workshops and 1:1 coaching\n      \n    \n    \n      \n        We believe the most impactful data analysis happens with strong subject matter intuition. There's no one better suited for this than your own people, but you need the technical foundation. Whether it's evaluating statistical methodology, integrating data science tools/programming languages, collaborative solution development, or traditional tutoring, we can work with you side by side to accelerate progress. \n      \n  \n\n\n\n\n\n\nLoading…"
  },
  {
    "objectID": "posts/simple-example-why-statistical-significance-is-insufficient/index.html",
    "href": "posts/simple-example-why-statistical-significance-is-insufficient/index.html",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "",
    "text": "When we see the phrase statistically significant, we’re often meant to believe it means that the result matters, but that is not the case. Here is a simple example why.\nWe decide to correlate the customer age with the sales amount. Suppose there are two scenarios from a sample of 500 customers:\nCode\n# Load packages\nlibrary(tidyverse)\n\n## Simulate some data\n\n# Set the seed for reproducibility\nset.seed(123456789)\n\n# Sample size\nn &lt;- 500\n\n# Create the data set\nsim_dat &lt;- \n  tibble(\n    Age = round(18 + rgamma(n = n, shape = 5, scale = 4)),\n    Sales_Large = 500 + 10 * (Age - mean(Age)) + rnorm(n = n, sd = 100),\n    Sales_Small = 500 + .10 * (Age - mean(Age)) + rnorm(n = n, sd = 1)\n  ) |&gt;\n  \n  # Send down the rows\n  pivot_longer(\n    cols = starts_with(\"Sales\"),\n    names_to = \"Effect\",\n    values_to = \"Sales\",\n    names_prefix = \"Sales_\"\n  ) \n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nAt a glance these graphs look very similar, such that age is positively correlated with the sales amount. We fit a linear regression model, or “best-fit line”, to summarize and describe the relationship.\nCode\n# Fit the models\nsim_models &lt;- \n  sim_dat |&gt;\n  \n  # Nest the data\n  nest(.by = Effect) |&gt;\n  \n  # Fit a linear model for each data set; get p-values\n  mutate(\n    \n    # Fit the model\n    model = \n      data |&gt;\n      map(\n        \\(.dat) \n        \n        lm(\n          formula = Sales ~ Age,\n          data = .dat\n        )\n        \n      ),\n    \n    # Compute the p-value\n    pvalue = \n      model |&gt;\n      map(\n        \\(.model) \n        2 * pt(\n          q = .model$coefficients / sqrt(diag(vcov(.model))), \n          df = .model$df.residual, \n          lower.tail = FALSE\n        )[[2]]\n      )\n  )\n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  geom_smooth(\n    aes(\n      x = Age,\n      y = Sales\n    ),\n    formula = y~x,\n    method = \"lm\",\n    color = \"black\",\n    se = FALSE\n  ) +\n  geom_text(\n    data = \n      sim_models |&gt;\n      \n      # Extract the p-value\n      unnest(cols = pvalue) |&gt;\n      \n      # Clean up p-value\n      mutate(\n        pvalue = \n          case_when(\n            pvalue &lt; 0.001 ~ \"&lt;0.001\",\n            TRUE ~ as.character(round(pvalue, 3))\n          )\n      ),\n    aes(\n      x = 60,\n      y = c(160, 496.5),\n      label = paste0(\"P-value: \", pvalue)\n    )\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nThe p-values for both of these models are extremely, and equally, small (&lt;0.1%), indicating statistical significance. In fact, the evidence is so strong that some might say it is very significant–much smaller than the standard (and infamous) rule of thumb threshold of 5%.\nIt is based on this information alone that often would elicit the conclusion/statement/finding that age is significantly associated with sales.\nYou hear this language all the time, especially in research. It brings with it certain implications of importance, as if it has now become a meaningful fact that should warrant attention and/or action.\nThe problem?\nWatch what happens when we add the actual scales for sales to these graphs (if you’ve noticed, they’ve been missing this whole time):\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = c(160, 496.5),\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect, scales = \"free_y\") +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n      name = \"Sales ($)\",\n      labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nOn the left panel, the range of sales goes from approximately $200 to $1000 per customer in an increasing fashion with age. On the right panel, it goes from about $497 to $503–a few dollars. See the issue yet? To solidify this, let’s look at the graphs when they are on the same scale:\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = 160,\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect) +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n        name = \"Sales ($)\",\n        labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nThe line in the right panel is basically flat.\nIt turns out that although these two graphs have the same amount of statistical significance, they clearly tell much different stories about how age relates to sales. The lines can be summarized as follows:\nIn the context of performing market segmentation for increased revenue (or whatever else it may be), these magnitudes certainly matter. Statistically, they don’t."
  },
  {
    "objectID": "posts/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "href": "posts/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "Some takeaways",
    "text": "Some takeaways\n\nStatistical significance only pertains to the existence of a relationship (under the implied assumptions), not the size of it.\nYou must pay attention to the magnitude of the relationship to gain any meaningful insight.\nThe magnitudes should be translated to the real-world implications of using the information for different decisions or courses of action. In the example above, the market age distribution looks like this:\n\n\n\nCode\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = Age\n    ),\n    fill = \"gray\",\n    color = \"black\",\n    alpha = .5\n  ) +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16)\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Customers\")\n\n\n\n\n\n\n\n\n\nAlthough older customers yield higher sales, it may cost more marketing dollars to acquire any given individual from such a small segment, ultimately making the juice not worth the squeeze. Feeding estimates into cost-benefit or what-if scenarios can greatly increase confidence in how a proposed course of action would actually play out, instead of implementing things on the basis of mere existence (i.e., statistical significance)."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Tutoring",
    "section": "",
    "text": "**Page under construction**\n\nTutoring\nWe offer tutoring, 1:1 coaching, and solution development in statistics and the R programming language. See our pricing tables below.\n\n\n\n\n  \n    \n      \n        Business Professionals\n      \n    \n    \n      \n         \n                \n                    \n                         Pay as you go \n                    \n                    \n                        \n                            $113per hour\n                        \n                    \n                \n                    \n                        \n                            10-19 hours (prepaid)\n                        \n                        \n                            \n                              $102per hour\n                            \n                        \n                    \n                                        \n                        \n                            20+ hours (prepaid)\n                        \n                        \n                            \n                              $97per hour\n                            \n                        \n                    \n            \n      \n    \n\n  \n    \n      \n        Students & Individuals\n      \n    \n    \n      \n                 \n                \n                    \n                         Pay as you go \n                    \n                    \n                        \n                            $75per hour\n                        \n                    \n                \n                    \n                        \n                            10-19 hours (prepaid)\n                        \n                        \n                            \n                              $68per hour\n                            \n                        \n                    \n                                        \n                        \n                            20+ hours (prepaid)\n                        \n                        \n                            \n                              $65per hour"
  },
  {
    "objectID": "posts/ways-to-ensure-success-of-statistical-project/index.html",
    "href": "posts/ways-to-ensure-success-of-statistical-project/index.html",
    "title": "5 ways to help ensure success of a statistical project",
    "section": "",
    "text": "Sometimes stats projects don’t go as planned. There are delays, setbacks, surprises, ambiguity, scope creep…the list goes on. All of these things can lead to a seemingly longer list of questions than what was started with: Did we answer the research question? Are we confident in the result? What do we do now? It’s a sense of dissatisfaction.\nMany of these issues stem from the early phases of the project, and (maybe I should collect data on this) they can largely be alleviated when more care is taken at that stage. Actually, the book I’m currently reading summed it up perfectly, “Well-designed research is research capable of answering the question it’s trying to answer”. Sounds obtuse, but it is undoubtedly true, even for projects outside of what you may define as “research”. This is about proper planning. So here are 5 things that can help increase the likelihood of a successful statistical project:\n\n1. Assemble your team…early\nIt’s often a misconception that the statistician’s role is to simply analyze, or “run the tests on”, the data at the end once it is collected. This is far from optimal. Statistical analysis is not systematic or mechanical. Rather, it requires knowledge and intuition about the subject matter context. Add in a lack of transparency to the data collection itself, the chances of lost insight definitely increase. In fact, it might be the case that a few poor design choices end up adding huge complexities in answering the original question, or maybe even make it impossible altogether. So, if you have a project idea, consult your statistician! Early and often during the development phase.\nNow, data people are certainly not the only ones you need involved early. Far from it. Who better to help shape the final product than the end-users—the people who will actually be using the information and know what works? If you want to integrate models into your operational workflow, what sort of resource or technical constraints may there be? Well, we probably have to talk to systems and IT people who will also likely need to commit their own resources for upkeep. And it comes full circle, because all of these nuances may even affect the statistical choices made from a mathematical perspective (i.e., design, modeling framework, etc.). There are plenty more important roles that could be described here, but the bottom line is that cross-functional collaboration, from the beginning, is crucial.\n\n\n2. Make the goals clear, then plan accordingly\nThere is a common issue in data projects of ambiguous or non-specific objectives. Statistics is inherently “gray”, by definition, because there is no right answer. Uncertainty always exists. So unless you already know exactly what you’re looking for in the data, without a clearly defined goal, you can find yourself spinning in circles and never know when what you’ve done is sufficient to move on.\nMy recommendation is to have multiple levels: the statistical goals and the real-world goals that they are supporting. The statistical goals should be stated as clear, specific questions with quantitative answers that data (among other things) will be used to estimate. However, we have to think about how these statistical quantities will be used afterwards. For example, the approach taken to estimate the likelihood of a customer buying a product (a statistical goal) may differ if we’re trying to decrease costs versus increase retention, especially when thinking about the solution as a whole. The question comes down to what we are trying to accomplish with the new information. Once that is clear, we can envision the roadmap for how it will be used, which can be an anchor for developing the right methodology, making analytic decisions easier to manage.\n\n\n3. Think about taking action\nOnce you obtain the new information you set out to find, what are you going to do about it? Under what circumstances? Based on which results? Having some inkling as to what it is going to enable (or disable) someone to do–not just in general, but a specific example–adds clarity to the practical implications of investing the time and money into finding the answers. These things can unravel many of the nuances that were originally an oversight, and may end up causing changes to how the information gets disseminated, who gets involved and when, or even the math itself. All in all, it allows for more proper design at the beginning, a reduction of wasted time/resources, and a better chance of finding the right solution. The reason we perform statistical analysis is (or should be) to inform some action or decision. If it doesn’t, then you may need to think about why that is and adjust. My favorite way to frame this to someone is by asking a simple question: “If you knew X at time Y then you could do Z. What are X, Y and Z?”.\n\n\n4. Create a tangible product\nIt may sound trivial, but it’s worth thinking about (and even explicitly defining). By what means will the result or solution be delivered? To whom? When? How? Is it going to be a comprehensive report or just a number sent in an email? It might even be a model deployed in the organization’s systems and workflows, or an application hosted on the web. These tell you all sorts of things about who should get involved (see #1) and what it will take to get there. It is about ensuring that the right information gets to the right people at the right time in an expected and predictable manner. It can be the case that a data project fails not because of bad statistics or models, but because it wasn’t disseminated optimally. Having a tangible end-product that you can work towards keeps everyone’s eye on the ball, exposes where the problems are, helps you plan deliverables, create milestones, and makes it clear when you are veering off course.\n\n\n5. Answer the question, “did it work?”\nOftentimes when you look at a statistic like a p-value, it leaves an empty feeling like you haven’t been convinced. That’s because it does not yet directly translate to the real-world impact the new insight is supposed to address. Maybe it’s useful during data analysis, but we should be thinking beyond the inferences made from the sample at hand to what we need to see to convince us that the results really matter. If the information we’ve garnered is actually useful, then we should expect improvements to play out where the rubber meets the road once it is utilized. The most direct way to do this: test it."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "",
    "text": "This dataset comes from the 2011 Annual Resident Survey in Durham, NC.\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Import the dataset\ndat &lt;- read_delim(\n  file = \"https://query.data.world/s/zr3uaxpaagbzddttoreosktj2zy7lm?dws=00000\", \n  delim = \";\",\n  na = c(\"\", \" \", \"NA\", \"N/A\")\n) |&gt;\n  \n  # Keep a few columns\n  transmute(\n    QOL = \n      q3f_quality_of_life_in_city |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Very Dissatisfied\",\n        \"Dissatisfied\",\n        \"Neutral\",\n        \"Satisfied\",\n        \"Very Satisfied\"\n      ),\n    Age = str_remove(`18_34_years`, \"(?i)\\\\syears$\") |&gt; factor(),\n    Income = \n      q38_annual_household_income |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Under $30,000\",\n        \"$30,000 to $59,999\",\n        \"$60,000 to $99,999\",\n        \"$100,000 or more\"\n      ),\n    Sex = q34_respondents_gender\n  ) |&gt;\n  \n  # Remove missing cases\n  na.omit()\nSuppose we are interested in understanding the relationship between resident age and their perceived quality of life in the city, after adjusting for gender and annual household income. We have the following observed distribution (note that we’ve removed missing data for simplicity):\nCode\ndat |&gt; \n  \n  # Compute group summaries\n  summarize(\n    N = n(),\n    .by = \n      c(\n        QOL,\n        Age\n      )\n  ) |&gt;  \n  \n  # Flip the order\n  mutate(\n    QOL = fct_rev(QOL)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() + \n  geom_col(\n    aes(\n      x = Age,\n      y = N,\n      fill = QOL\n    ),\n    color = \"black\",\n    alpha = .75\n  ) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(hjust = .5)\n  ) +\n  xlab(\"Respondent Age (years)\") +\n  ylab(\"Count\") +\n  labs(title = \"Quality of life in the city\")\nOverall, it looks like older respondents tend to report more pessimistic views of quality of life."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "What does that mean?",
    "text": "What does that mean?\nLet’s clarify this by using our model output directly. We’ll fit the model, adjusted for annual household income and gender, using the MASS::polr function. The age group 18-34 will serve as the reference category in which all other age groups will be compared against. Note: By default the package computes odds ratios in the opposite direction to what we what we want, so we invert them.\n\n\nCode\n# Fit the model\nmod &lt;-\n  MASS::polr(\n    formula = QOL ~ Age + Income + Sex,\n    data = dat,\n    Hess = TRUE\n  )\n\n# Make a table of odds-ratios\nmod$coefficients |&gt;\n  \n  # Convert to data frame\n  enframe(\n    name = \"Term\",\n    value = \"Estimate\"\n  ) |&gt;\n  \n  # Join to get the CI\n  inner_join(\n    y = \n      # Get the 95% confidence intervals\n      confint(mod) |&gt; \n      \n      # Convert to tibble, add the coefficient names\n      as_tibble() |&gt; \n      add_column(Term = names(mod$coefficients)),\n    by = \"Term\"\n  ) |&gt; \n  \n  # Filter to age factor only\n  filter(str_detect(Term, \"^Age\")) |&gt;\n  \n  # Clean up\n  mutate(\n    Term = str_remove(Term, \"^Age\"),\n    across(\n      where(is.numeric),\n      \\(x) sprintf(\"%.2f\", 1 / exp(x))\n    )\n  ) |&gt; \n  \n  # Rename\n  rename(\n    `Age (years)` = Term,\n    `Odds-ratio` = Estimate,\n    Lower = `97.5 %`,\n    Upper = `2.5 %`\n  ) |&gt;\n  \n  # Change location\n  relocate(Lower, .before = Upper) |&gt;\n  \n  # Add the reference row\n  add_row(\n    `Age (years)` = \"18-34\",\n    `Odds-ratio` = \"-\",\n    Lower = \"-\",\n    Upper = \"-\",\n    .before = 1\n  ) |&gt;\n  \n  # Make a table\n  knitr::kable(format = \"html\") |&gt;\n  kableExtra::kable_styling() |&gt;\n  kableExtra::add_header_above(c(\"\", \"\", \"95% CI\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% CI\n\n\n\nAge (years)\nOdds-ratio\nLower\nUpper\n\n\n\n\n18-34\n-\n-\n-\n\n\n35-44\n1.24\n0.67\n2.30\n\n\n45-54\n1.13\n0.64\n1.98\n\n\n55-64\n1.74\n0.97\n3.12\n\n\n65-74\n3.02\n1.40\n6.54\n\n\n75+\n0.93\n0.30\n2.88\n\n\n\n\n\n\n\n\nGenerally, the estimates pan out roughly how we suspected. In particular, the estimated odds of worse perceived quality of life in the city for 65-74 year olds are 3 times that of 18-34 year olds, after adjusting for annual household income and gender (with a 95% confidence interval of 1.4 to 6.5).\nAgain, this interpretation is assumed to hold true if “worse” is defined as very dissatisfied versus everything else, or very dissatisfied through satisfied versus very satisfied, and everything in between."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Continue…",
    "text": "Continue…\nWe said earlier that the model assumes the same odds ratios for any mutually exclusive comparison of the ordered response categories. Thus, we can free up this constraint by thinking of constructing a collection of binary logistic regression models: one for each of those ordinal comparisons. Specifically,\n\nVery Dissatisfied versus everything else\nVery Dissatisfied or Dissatisfied versus everything else\nVery Dissatisfied, Dissatisfied, or Neutral versus Satisfied or Very Satisfied\nVery Dissatisfied through Satisfied versus Very Satisfied\n\nThen, we simply just look to see if the resulting odds ratios are reasonably similar across all of those models. If so, then we can be somewhat confident that it can be reduced to a single model, and stick with our original proportional-odds estimates.\nMy preference is to do this in a plot."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Making the plot",
    "text": "Making the plot\nWe’ll cycle through the response categories, iteratively define the binary outcomes as described above, and then fit a logistic regression model for each definition. Once we do this, we get the following plot:\n\n\nCode\n# Set the number of comparisons\nn_comp &lt;- n_distinct(dat$QOL) - 1\n\n# Make each data set\n1:n_comp |&gt;\n  \n  # For each set\n  map_df(\n    function(.index) {\n      \n      # Extract the current response set\n      temp_resp &lt;- levels(dat$QOL)[1:.index]\n      \n      # Create binary outcome in the data\n      temp_dat &lt;- \n        dat |&gt;\n        \n        # Create target\n        mutate(\n          Response = \n            case_when(\n              QOL %in% temp_resp ~ 1,\n              TRUE ~ 0\n            )\n        )\n      \n      # Fit the binary logistic regression model\n      temp_mod &lt;-\n        glm(\n          formula = Response ~ Age + Income + Sex,\n          data = temp_dat,\n          family = \"binomial\"\n        )\n      \n      # Make a table of odds-ratios\n      temp_mod$coefficients |&gt;\n        \n        # Convert to data frame\n        enframe(\n          name = \"Term\",\n          value = \"Estimate\"\n        ) |&gt;\n        \n        # Join to get the CI\n        inner_join(\n          y = \n            # Get the 95% confidence intervals\n            confint.default(temp_mod) |&gt; \n            \n            # Convert to tibble, add the coefficient names\n            as_tibble() |&gt; \n            add_column(Term = names(temp_mod$coefficients)),\n          by = \"Term\"\n        ) |&gt; \n        \n        # Filter to age factor only\n        filter(str_detect(Term, \"^Age\")) |&gt;\n        \n        # Clean up\n        mutate(\n          Term = str_remove(Term, \"^Age\"),\n          across(\n            where(is.numeric),\n            \\(x) exp(x)\n          )\n        ) |&gt;\n        \n        # Rename\n        rename(\n          Age = Term,\n          OR = Estimate,\n          Lower = `2.5 %`,\n          Upper = `97.5 %`\n        ) |&gt;\n        \n        # Add the reference row\n        add_row(\n          Age = \"18-34\",\n          OR = 1,\n          Lower = 1,\n          Upper = 1,\n          .before = 1\n        ) |&gt;\n        \n        # Attach outcome level\n        add_column(QOL = levels(dat$QOL)[.index])\n      \n    },\n    .id = \"Order\"\n  ) |&gt;\n  \n  # Make the factor\n  mutate(\n    Order = as.numeric(Order),\n    QOL = factor(QOL) |&gt; fct_reorder(Order)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = QOL,\n      y = OR,\n      group = 1\n    )\n  ) +\n  geom_line(\n    aes(\n      color = Age\n    ),\n    linewidth = 1.5\n  ) +\n  geom_point(\n    aes(\n      color = Age\n    ),\n    size = 3\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = Lower,\n      ymax = Upper,\n      fill = Age\n    ),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 1, color = \"gray\") +\n  facet_wrap(~paste0(Age, \" years\")) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"none\",\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, size = 10),\n    plot.title = element_text(hjust = .5),\n    strip.text = element_text(size = 14)\n  ) +\n  xlab(\"Response\") +\n  ylab(\"Odds-ratio (95% CI) for being at or below response level\")\n\n\n\n\n\n\n\n\n\nUnfortunately we’re quite plagued by variability here, especially in the lower-end (i.e., very dissatisfied versus everything else), due to scanty event volumes, but you get the picture. Actually, for 65-74 year olds, the proportional-odds assumption seems to be a reasonable one: it was estimated earlier at 3.02, and we see the point estimates across these binary models vary between 2.5-3.5.\nFor other age categories, it may not be so good of an assumption. It looks like 35-44 and 55-64 year olds tend to have a much higher odds of responding very dissatisfied relative to 18-34 year olds, but there is much less of a difference (in all age categories) for the odds of responding very satisfied, suggesting something like older residents may make a point to select the least favorable response but don’t see much difference between being satisfied or very satisfied."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "So what do we do in practice?",
    "text": "So what do we do in practice?\nFirst, the same proportional-odds assumptions hold for all covariates in the model, so we would also want to assess this for annual household income and gender. Second, if the assumption is not met, then we need to accommodate that by introducing more flexibility into the model. That may be by being clever with interaction terms, defining sensible groups to create, or by using separate binary models for each possible comparison, as we’ve done here. It’s really a judgement call."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Alex is statistician and owner at CentralStatz Statistical & Data Sciences LLC from Wausau, WI. He has over a decade of experience learning, teaching, and doing data science work with a wide range of tools, methodologies and contexts. His primary focus is on delivering genuine value through the use of statistics and empowering others to do the same. Want to chat? Shoot him an email!\n\nalex@centralstatz.com"
  },
  {
    "objectID": "about.html#alex-zajichek",
    "href": "about.html#alex-zajichek",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Alex is statistician and owner at CentralStatz Statistical & Data Sciences LLC from Wausau, WI. He has over a decade of experience learning, teaching, and doing data science work with a wide range of tools, methodologies and contexts. His primary focus is on delivering genuine value through the use of statistics and empowering others to do the same. Want to chat? Shoot him an email!\n\nalex@centralstatz.com"
  }
]