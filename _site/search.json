[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCan you have a model without data?\n\n\n\n\n\n\nBayesian\n\n\n\nYes, by being a Bayesian.\n\n\n\n\n\nOct 25, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nYou should have a data science blog\n\n\n\n\n\n\nDeployment\n\n\nLearning\n\n\nSoftware Development\n\n\n\nEasy and free are only a couple benefits.\n\n\n\n\n\nSep 25, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nLow cost ways to build and deploy analytical web apps\n\n\n\n\n\n\nWeb applications\n\n\nDeployment\n\n\n\nAn overview of some server options\n\n\n\n\n\nAug 20, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you assess the proportional-odds assumption? Directly.\n\n\n\n\n\n\nRegression Modeling\n\n\n\nA simple visual check.\n\n\n\n\n\nJul 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA simple example why statistical significance is insufficient for action\n\n\n\n\n\n\nStatistical Significance\n\n\nDecision Making\n\n\n\nIt ignores the basic question: “How much?”\n\n\n\n\n\nJun 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\n5 ways to help ensure success of a statistical project\n\n\n\n\n\n\nProject Management\n\n\n\nHow can we increase the likelihood that things will go as expected?\n\n\n\n\n\nMay 16, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/your-organization-should-be-using-r-shiny/index.html",
    "href": "presentations/your-organization-should-be-using-r-shiny/index.html",
    "title": "Your organization should be using R Shiny",
    "section": "",
    "text": "In progress\n\nSlides\n\n\nLink to slideshow"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Alex is statistician and owner at CentralStatz Statistical & Data Sciences LLC from Wausau, WI. He has over a decade of experience learning, teaching, and doing data science work with a wide range of tools, methodologies and contexts. His primary focus is on delivering genuine value through the use of statistics and empowering others to do the same. Want to chat? Shoot him an email!\n\nalex@centralstatz.com"
  },
  {
    "objectID": "about.html#alex-zajichek",
    "href": "about.html#alex-zajichek",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Alex is statistician and owner at CentralStatz Statistical & Data Sciences LLC from Wausau, WI. He has over a decade of experience learning, teaching, and doing data science work with a wide range of tools, methodologies and contexts. His primary focus is on delivering genuine value through the use of statistics and empowering others to do the same. Want to chat? Shoot him an email!\n\nalex@centralstatz.com"
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "",
    "text": "In the era of artificial intelligence (AI), organizations are often told they must buy in or get left behind. I’ve attended various presentations like this. The presenters discuss how things are changing, provide some high-level overview of AI, and then convey that the organization must develop a strategy for it. However, the feeling left in the room is usually that of confusion and ambiguity. They understand that they should be thinking about it, but it’s totally unclear what (if anything) they should actually do.\nFirst, it’s not really clear what is meant by “AI”, as it can mean many things (I often don’t know what they mean by it either). Second, the implication seems to be pointed toward AI in the context of tools that can be purchased from vendors, therefore adding a financial stressor to decision makers when figuring out how to act. It also conveys a one-sided message: that AI is products that you buy, never really discussing the technical foundation, and thus foregoing the idea of taking these concepts and building incrementally.\nSure, there are all sorts of obscure ways organizations can use tools like ChatGPT, for example, but I would venture to guess that in the bigger picture, much of the target audience is at a relatively early stage of their data science journey, and unaware of all of the opportunity that is available using low-cost, or free, programming languages and technologies that can get them well on their way to a mature advanced analytics function, without making a direct jump to cost-intensive solutions in hopes that they “work”. There’s a better way.\nData science is all about asking the right questions and figuring out ways to provide answers to those questions to the people who need them at the right time so they can take action. This is somewhat arbitrary, but that also means it leaves the door open to an infinite number of ways to develop solutions for it, many of which can be solved (a) by going back to the fundamentals: data quality, data collection, data storage, infrastructure, reporting workflows, etc., and (b) with, at least to start, freely-available software, tools and programming languages. AI is simply hyperbole for data and analytical strategy. There’s lots of ways to start using data better, and that doesn’t necessarily mean opening your wallet."
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#locally",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#locally",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "1. Locally",
    "text": "1. Locally\nWe already said that the primary goal of sharing an application is to get it off of your local machine, but this is still worth mentioning, because it is how you start. Once you install R, RStudio, and the shiny package, you’ll have everything you need to begin developing and running apps. This is of course a great way to develop, and often the way you would do it regardless, because you can write the code, run the app to see how it works, make changes, test, and repeat.\nBut once it’s finalized and you want to share it, without other infrastructure in place, you would need to send the code files to someone else and have them run it on their own machine, which means they need to have all the software installed as well. This can be a totally legitimate approach in certain cases. It is certainly a starting point, but it’s far from ideal.\n\nSide note\nAnother reason I mention local “deployment” is because I actually use this approach quite often. When I am analyzing a dataset, it is extremely useful to be able to spin up an application for my own usage to easily explore certain aspects of it, among many other uses. In this case, the audience is me, and I can make it work however I need it to serve my purposes. You can do this too, and don’t need it to run anywhere but your own computer, yet it is still very useful."
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#shinyapps",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#shinyapps",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "2. shinyapps.io",
    "text": "2. shinyapps.io\nThe most obvious location to deploy a Shiny app is the place specifically purposed for it: shinyapps.io. It is a cloud-based server built and maintained by Posit (who we’ll see come up a lot in this article) that is made for hosting apps built in this framework. It has been around for quite a while, and, as we’ll see, there have been other platforms developed for hosting these apps that may be a better fit. Nevertheless, this is a totally viable option and great way to start deploying things, and they have a free tier that allows you to deploy up to five (5) applications with 25 hours of active usage per month.\n\nThe Basics\nThe concept is simple: develop your app on your local machine (e.g., in RStudio) and then run the rsconnect::deployApp function to deploy it to shinyapps.io. You can create an account with your email address or sign up through GitHub (like I did), among other options. Here is an outline of the steps I took to deploy this app:\n\n1. Make an application\nFirst you just need to code up an app. This could be very simple like the one found here to get started.\nI’m using an existing demo app we have in our GitHub page. From Terminal we can execute (and you can too):\ngit clone https://github.com/centralstatz/ExampleApps.git\nThe app we want is located at ExampleApps/ReadmissionRiskPool.\n\n\n2. Configure connection to shinyapps.io\nWhen you sign up, you’ll get a basic list of instructions for getting started. Assuming you are doing it with R, you need to install the rsconnect R package and register your account information, which includes tokens/secrets that are provided with your shinyapps.io account.\n# Install the configuration library\ninstall.packages(\"rsconnect\")\n\n# Setup account information\nmy_user_name &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nmy_token &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nmy_secret &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nrsconnect::setAccountInfo(\n  name = my_user_name,         \n  token = my_token,     \n  secret = my_secret\n)\n\n\n3. Deploy the application\nOnce your account is configured, you can call rsconnect::deployApp() with the application specified and it will be uploaded to the host server.\n# Assume we're working in the repo just cloned\nrsconnect::deployApp(\"ReadmissionRiskPool\")\nAnd just like that we have a custom, interactive web application available to the world.\nOne thing you might notice is the web URL: https://tgzz86-alex0zajichek.shinyapps.io/readmissionriskpool/\nIt is obviously not very nice looking. One way to clean this up would be to have a better username (the first part of it). Then, once you start getting into the shinyapps.io paid plans, they allow for custom domains.\nAnother part of this is security. The app we just deployed is publicly available, which is definitely not always what is desired. Again, with the paid tiers you can begin introducing authentication into the applications for restricted access."
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#posit-cloud",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#posit-cloud",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "3. Posit Cloud",
    "text": "3. Posit Cloud\nThis platform takes it up a notch. Posit Cloud, developed and maintained by the same company, is not only a place to share applications, but a cloud-based service where you can develop your code as well, so everything is accessed in your web browser. As the website describes it is a great platform for collaboration, teaching, etc. in a more all-encompassing environment.\nYou can organize content into “spaces”, which are disjointed areas for separate work streams. Within each space, and at the content item level, you can control settings for who has access, to what, and how much. Each user creates a login to the platform, and is able to access the spaces that they have created or have been shared with them. The free tier enables you a single shared space (which you can invite other users to) with up to 25 projects/outputs and limited computation usage. There is also infrastructure in place to establish connections to databases, among other things, that make it a feasible home for full-fledged data solutions. The other very important thing to note is that it is not only Shiny applications that can be deployed here, but all sorts of other analytical content such as R Markdown and Quarto documents, API’s, etc. Here is an introductory video for more information straight from the source:\n\n\nHow we use Posit Cloud\nWith the paid plans of Posit Cloud, you are allowed an unlimited number of spaces, as well as beefed-up compute. This is what enables us to take advantage of this platform as an offering for client engagements. Each client gets their own private space which all of their analytical content we are collaborating on lives, and only we (us and the client) have access. Here’s one possible example of how a client engagement could work:\n\nA client reaches out because they want to build a web application that enables them to interactively explore data related to their customer base on a reactive map, among other functionality.\nWe initialize a new space in Posit Cloud and invite the client via email. They receive the invitation link that takes them to the space upon login, creating an account if they have not done so already.\nWe initialize a new private (or public, if applicable) GitHub repository on our organization’s page to hold all of the application’s source code and its change-tracking history. Optionally, the client can be added as a member with viewing privileges of these files as well for full transparency.\nWe initialize a new RStudio project within the workspace sourced from the created GitHub repository. This serves as the development environment for the application.\nThe client has a dataset in a large Excel file that we would like to use as the source for the application. It gets uploaded into the application’s project by us after the client sends it in an email (or alternatively, the client goes in and uploads it themselves).\nThe app development work begins. We work iteratively with the client to construct it most optimally to fit their needs (occassionally pushing code to GitHub to track changes). Because they have direct access to the space, they can see it anytime. We can quickly show progress updates, bounce ideas back and forth, test functionality, and answer questions in a timely manner, until it is satisfactory. Then, we deploy the final application, and the client is enabled to go into it on-demand and use it as seen fit.\nOver time, the data becomes stale so the client would like it to be updated on a recurring monthly basis. One option would be to send a new Excel file each month and we will manually update and re-deploy the application. Instead, we decide to use Posit Cloud’s built-in data integration capabilities. So we establish a connection to a SQL database in which the Excel file was sourced from, and build the queries directly into the application’s source code so that the application itself is sourced directly from the database. No middleman required.\n\nOverall, Posit Cloud is a highly recommended tool to use for individuals and/or teams of people to get started with application development and deployment. Or even if you’re a seasoned Shiny developer, the infrastructure you get out of the box is amazing. And again, it is free."
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#connect-cloud",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#connect-cloud",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "4. Connect Cloud",
    "text": "4. Connect Cloud\nPosit just recently launched the Alpha release of their newest platform, called Connect Cloud. It is in its early phase, and it’s all about easy deployment. As the home page states, there are three (3) steps to get an application up and running with a shareable link that can be accessed from anywhere:\n\nCreate a new account by authenticating with GitHub (so you must have a GitHub account)\nLink to a public repository (from GitHub) containing the code for a Shiny application (or whatever other type of content you’re deploying)\nDeploy the application and share the link\n\nWe did this with the application deployed in #2, and yes, it’s that easy (and, once again, free). The link for the app on this platform is here.\nThe main thing you have to remember to do before making your final commit to GitHub, and subsequently configuring the app connection to the repository, is to create the manifest.json file in your application’s root directory to capture the environment parameters that need to be created (automatically by Connect Cloud) for it to run. This can be done with a simple command:\nrsconnect::writeManifest()\nYou can watch this video to get a more thorough introduction:\n\nWe don’t quite yet know where this platform is going to go given that it is in such early stages. Although the infrastructure it already provides, and the ease at which it is done, is magic-like, there are of course a lot of things it doesn’t have (yet): login without GitHub, source apps from private repositories, private content, authentication, security, etc. These were the things I wondered about when I attended the live webinar above, which in turn had me thinking how it exactly compares with the purpose of Posit Cloud (in the video, I explicitly asked this at 29:26). It sounds like these “commercial” considerations are all part of the roadmap, so we are very excited to see where it goes.\nNevertheless, even in its current state, Connect Cloud is a highly recommended platform to start using for anyone wanting to build a data science portfolio and deploy public content."
  },
  {
    "objectID": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#shiny-server",
    "href": "posts/low-cost-ways-to-build-and-deploy-apps/index.html#shiny-server",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "5. Shiny Server",
    "text": "5. Shiny Server\nWant total control over the infrastructure? If yes, Shiny Server might be for you.\nThis is an open-source server configuration that can be installed on-premises (or wherever you’d like). The huge advantage is that it is always free–you just need to install the software. The disadvantage is that you need a server bulky enough to handle the desired compute and you need people who know how to manage it. Compared to the others, this option is like the wild west. It provides you the basic skeleton to get stuff working, but from there you have ultimate freedom to do with it what you wish, which can easily get out of hand as the developer/user bases grow and you’re trying to maintain environments as new software package versions are constantly being released.\nI always consider this a fantastic option for relatively large organizations who are early in their data science journeys. They likely have mature information technology (IT) teams supporting existing systems, but have not yet invested heavily in advanced analytics infrastructure. This provides an opportunity to leverage those mature systems admin teams to setup up and manage the backend of this infrastructure that then enables data scientists in the organization to deliver great analytic content (and prove its value) without spending tons of money on acquiring the software. Low risk with huge potential.\n\nAmazon Web Services (AWS)\nDespite it being open-source, most individuals (or small companies for that matter) probably don’t have adequate server space to implement Shiny Server in the way they’d like. However, you can do it if you use external compute resources. Amazon EC2 is one way to make it happen. And still for free.\nThe way it works is that you spin up an EC2 instance of your choice (essentially a computer, ranging from very low to very high compute power, the t2.micro being the one that you can use for free), install Shiny Server on the instance, and then have all the freedom in the world to deploy applications to it and access them on the web. This is an excellent resource that I followed when learning how to do this, which you can see a detailed account of my steps here as well.\nThe real cool thing about this setup is that you can take it wherever you want to go. You have unlimited ability to customize the design of your server pages, integrate it with other software/tools, assign custom domains, etc. The possibilities are endless. When I learned how to do this, I was able to quickly get it to a state where my server is live and accessible at a subdomain of one of my websites: http://apps.zajichekstats.com/. Clicking that link takes you to the home page of my Shiny Server on an AWS EC2 t2.micro instance, where subsequent applications are found at subpages of that (e.g., http://apps.zajichekstats.com/shinydemo-aws/). This entire setup, including the subdomain assignment, was done for free, and it is barely scratching the surface of what can be done with it."
  },
  {
    "objectID": "posts/simple-example-why-statistical-significance-is-insufficient/index.html",
    "href": "posts/simple-example-why-statistical-significance-is-insufficient/index.html",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "",
    "text": "When we see the phrase statistically significant, we’re often meant to believe it means that the result matters, but that is not the case. Here is a simple example why.\nWe decide to correlate the customer age with the sales amount. Suppose there are two scenarios from a sample of 500 customers:\nCode\n# Load packages\nlibrary(tidyverse)\n\n## Simulate some data\n\n# Set the seed for reproducibility\nset.seed(123456789)\n\n# Sample size\nn &lt;- 500\n\n# Create the data set\nsim_dat &lt;- \n  tibble(\n    Age = round(18 + rgamma(n = n, shape = 5, scale = 4)),\n    Sales_Large = 500 + 10 * (Age - mean(Age)) + rnorm(n = n, sd = 100),\n    Sales_Small = 500 + .10 * (Age - mean(Age)) + rnorm(n = n, sd = 1)\n  ) |&gt;\n  \n  # Send down the rows\n  pivot_longer(\n    cols = starts_with(\"Sales\"),\n    names_to = \"Effect\",\n    values_to = \"Sales\",\n    names_prefix = \"Sales_\"\n  ) \n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nAt a glance these graphs look very similar, such that age is positively correlated with the sales amount. We fit a linear regression model, or “best-fit line”, to summarize and describe the relationship.\nCode\n# Fit the models\nsim_models &lt;- \n  sim_dat |&gt;\n  \n  # Nest the data\n  nest(.by = Effect) |&gt;\n  \n  # Fit a linear model for each data set; get p-values\n  mutate(\n    \n    # Fit the model\n    model = \n      data |&gt;\n      map(\n        \\(.dat) \n        \n        lm(\n          formula = Sales ~ Age,\n          data = .dat\n        )\n        \n      ),\n    \n    # Compute the p-value\n    pvalue = \n      model |&gt;\n      map(\n        \\(.model) \n        2 * pt(\n          q = .model$coefficients / sqrt(diag(vcov(.model))), \n          df = .model$df.residual, \n          lower.tail = FALSE\n        )[[2]]\n      )\n  )\n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  geom_smooth(\n    aes(\n      x = Age,\n      y = Sales\n    ),\n    formula = y~x,\n    method = \"lm\",\n    color = \"black\",\n    se = FALSE\n  ) +\n  geom_text(\n    data = \n      sim_models |&gt;\n      \n      # Extract the p-value\n      unnest(cols = pvalue) |&gt;\n      \n      # Clean up p-value\n      mutate(\n        pvalue = \n          case_when(\n            pvalue &lt; 0.001 ~ \"&lt;0.001\",\n            TRUE ~ as.character(round(pvalue, 3))\n          )\n      ),\n    aes(\n      x = 60,\n      y = c(160, 496.5),\n      label = paste0(\"P-value: \", pvalue)\n    )\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nThe p-values for both of these models are extremely, and equally, small (&lt;0.1%), indicating statistical significance. In fact, the evidence is so strong that some might say it is very significant–much smaller than the standard (and infamous) rule of thumb threshold of 5%.\nIt is based on this information alone that often would elicit the conclusion/statement/finding that age is significantly associated with sales.\nYou hear this language all the time, especially in research. It brings with it certain implications of importance, as if it has now become a meaningful fact that should warrant attention and/or action.\nThe problem?\nWatch what happens when we add the actual scales for sales to these graphs (if you’ve noticed, they’ve been missing this whole time):\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = c(160, 496.5),\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect, scales = \"free_y\") +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n      name = \"Sales ($)\",\n      labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nOn the left panel, the range of sales goes from approximately $200 to $1000 per customer in an increasing fashion with age. On the right panel, it goes from about $497 to $503–a few dollars. See the issue yet? To solidify this, let’s look at the graphs when they are on the same scale:\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = 160,\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect) +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n        name = \"Sales ($)\",\n        labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nThe line in the right panel is basically flat.\nIt turns out that although these two graphs have the same amount of statistical significance, they clearly tell much different stories about how age relates to sales. The lines can be summarized as follows:\nIn the context of performing market segmentation for increased revenue (or whatever else it may be), these magnitudes certainly matter. Statistically, they don’t."
  },
  {
    "objectID": "posts/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "href": "posts/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "Some takeaways",
    "text": "Some takeaways\n\nStatistical significance only pertains to the existence of a relationship (under the implied assumptions), not the size of it.\nYou must pay attention to the magnitude of the relationship to gain any meaningful insight.\nThe magnitudes should be translated to the real-world implications of using the information for different decisions or courses of action. In the example above, the market age distribution looks like this:\n\n\n\nCode\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = Age\n    ),\n    fill = \"gray\",\n    color = \"black\",\n    alpha = .5\n  ) +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16)\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Customers\")\n\n\n\n\n\n\n\n\n\nAlthough older customers yield higher sales, it may cost more marketing dollars to acquire any given individual from such a small segment, ultimately making the juice not worth the squeeze. Feeding estimates into cost-benefit or what-if scenarios can greatly increase confidence in how a proposed course of action would actually play out, instead of implementing things on the basis of mere existence (i.e., statistical significance)."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html",
    "href": "posts/you-should-have-a-data-science-blog/index.html",
    "title": "You should have a data science blog",
    "section": "",
    "text": "One thing I would highly recommend to anyone with an interest in statistics/data science, whether you’re a student just starting out or a professional increasing your skills, is to create a blog. In this blog post, we’ll cover some of the benefits of doing so and touch on our preferred workflow to get one up and running (for free)."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#software-developmentdeployment",
    "href": "posts/you-should-have-a-data-science-blog/index.html#software-developmentdeployment",
    "title": "You should have a data science blog",
    "section": "1. Software development/deployment",
    "text": "1. Software development/deployment\nYou might think that the logistics/maintenance of a blog sounds cumbersome. You just want to stick to the content. However, learning how to create, develop, maintain, and deploy the blog, regardless of its content, provides in itself exposure to essential skills for data science: software development and deployment. Simply knowing the math or the R/Python commands to run a model is one thing, but if you want to deliver effective analytical solutions, those results probably need to be delivered to the end user in a useful way. Building a blog (website) forces you to learn about things such as version control, CI/CD, web hosting, DNS records, web development…the list goes on, which are all components of creating a data science product. Marrying these things to the content creation itself provides you an arsenal of tools that can be exploited in all sorts of contexts."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#solidify-concepts",
    "href": "posts/you-should-have-a-data-science-blog/index.html#solidify-concepts",
    "title": "You should have a data science blog",
    "section": "2. Solidify concepts",
    "text": "2. Solidify concepts\nThere are a lot of topics to keep track of when you’re learning statistics/data science, and one very effective way to solidify your understanding is to write it out coherently. For example, you may be learning about linear regression but there may be nuances surrounding it that are fuzzy, such as why a \\(\\beta\\) parameter is interpreted the way that it is. So, you may opt to write a blog post that goes through this derivation in an applied example with a coherent, structured narrative–by the end, you’ll generally grasp the thing you set out for. You just need to start with an outline of thing you want to understand and then learn it as you write. This is exactly the type of thing I do as well when I want to fully vet my understanding of something, such as in this post. Not only does this help with becoming a better writer, which is a great skill in and of itself, but it forces you to articulate a topic thoroughly, as if someone else is going to read it (which they hopefully will!). More importantly, it allows you to have permanent, easily accessible place to put your work."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#reference-repository",
    "href": "posts/you-should-have-a-data-science-blog/index.html#reference-repository",
    "title": "You should have a data science blog",
    "section": "3. Reference repository",
    "text": "3. Reference repository\nOn a related note of conceptual understanding: your blog posts are now just public web pages on the internet. That means you can use your collection of articles simply as an accessible repository for you to refer back to when you need them. If you took the time to write an article to help yourself solidify a concept (#2), then it was probably (a) tricky enough that over time you may lose your intuition on it occasionally, and (b) important enough that it was worth writing out. So, having a place where you’ve written out your thought process about a topic, in your own words, is an invaluable resource for you to refer back to. I can’t tell you the number of times I’ve referred back to articles I’ve written to remember little things. The beauty of it is that when my curiosity comes at a random part of the day, I know exactly where to look, and I can whip out my phone, remember the thing, and then stop thinking about it."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#portfolio-to-showcase",
    "href": "posts/you-should-have-a-data-science-blog/index.html#portfolio-to-showcase",
    "title": "You should have a data science blog",
    "section": "4. Portfolio to showcase",
    "text": "4. Portfolio to showcase\nThe fact that you even have a website is impressive. It shows that you can figure out how to do things, but more importantly, that you have the drive/initiative to create projects related to your craft. The skills in creating and maintaining the website are undoubtedly transferable to the work you’d do in data science. Add in interesting content you are producing in the blog posts themselves, such as code tutorials, analyses, method exploration, whatever it may be, then you have something that can prove your capabilities, and set you apart from other candidates. The convenience and security of having that all accessible through a simple web URL that you can quickly share with someone is a real advantage, as if someone asks you for work samples or something like that, you can confidently respond knowing the work was already done."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#engaging-in-public-discourse",
    "href": "posts/you-should-have-a-data-science-blog/index.html#engaging-in-public-discourse",
    "title": "You should have a data science blog",
    "section": "5. Engaging in public discourse",
    "text": "5. Engaging in public discourse\nHaving your own blog means you can write about whatever you’d like, however you want to say it, providing an avenue to contribute your own unique thoughts and perspectives. You may read a book or another article on some data science (or other) topic, and have strong thoughts, questions, or opinions about what was said. Or you just want to dig a little deeper into a certain aspect of it. Or reframe it in a way that is more understandable for yourself. Write it out and share it! Others may find your take interesting (or not, who cares). It only adds thought diversity, and makes you more connected to your field, as a peer."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#install-software",
    "href": "posts/you-should-have-a-data-science-blog/index.html#install-software",
    "title": "You should have a data science blog",
    "section": "1. Install software",
    "text": "1. Install software\nI prefer to use RStudio as my IDE for developing my websites, so you’ll also need to install R. Then, you can install Quarto and you’ll have what you need."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#store-code-on-github",
    "href": "posts/you-should-have-a-data-science-blog/index.html#store-code-on-github",
    "title": "You should have a data science blog",
    "section": "2. Store code on GitHub",
    "text": "2. Store code on GitHub\nThe source code for the blog should be version controlled and stored in a remote repository. I prefer GitHub. It’s not actually necessary to do this, but creates a much cleaner workflow and promotes better software development practices, as it retains all history to your code changes. As you develop your website locally, you’ll commit and push changes as you get to good stopping points, and GitHub will serve as your website’s source of truth."
  },
  {
    "objectID": "posts/you-should-have-a-data-science-blog/index.html#host-your-site-on-netlify",
    "href": "posts/you-should-have-a-data-science-blog/index.html#host-your-site-on-netlify",
    "title": "You should have a data science blog",
    "section": "3. Host your site on Netlify",
    "text": "3. Host your site on Netlify\nYour code is on GitHub, but you need a server that will actually host the live application. That is where Netlify comes in, where you can host your website for free. To make it very easy, you can configure Netlify to update your website everytime code changes are made to GitHub (#2), automatically. So when I develop my website (like this very blog post), I just commit and push my changes to GitHub (from RStudio) and that will automatically trigger Netlify to grab changes from the repository and update the live website. By default, your website URL will be something like website.netlify.app, but Netlify also has a great domain management system where you can point your website to a custom URL that you own (which you’ll likely have to pay for). Nevertheless, if you don’t mind the default URL, it is completely free."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Tutoring",
    "section": "",
    "text": "**Page under construction**\n\nTutoring\nWe offer tutoring, 1:1 coaching, and solution development in statistics and the R programming language. See our pricing tables below.\n\n\n\n\n  \n    \n      \n        Business Professionals\n      \n    \n    \n      \n         \n                \n                    \n                         Pay as you go \n                    \n                    \n                        \n                            $113per hour\n                        \n                    \n                \n                    \n                        \n                            10-19 hours (prepaid)\n                        \n                        \n                            \n                              $102per hour\n                            \n                        \n                    \n                                        \n                        \n                            20+ hours (prepaid)\n                        \n                        \n                            \n                              $97per hour\n                            \n                        \n                    \n            \n      \n    \n\n  \n    \n      \n        Students & Individuals\n      \n    \n    \n      \n                 \n                \n                    \n                         Pay as you go \n                    \n                    \n                        \n                            $75per hour\n                        \n                    \n                \n                    \n                        \n                            10-19 hours (prepaid)\n                        \n                        \n                            \n                              $68per hour\n                            \n                        \n                    \n                                        \n                        \n                            20+ hours (prepaid)\n                        \n                        \n                            \n                              $65per hour"
  },
  {
    "objectID": "posts/can-you-have-a-model-without-data/index.html",
    "href": "posts/can-you-have-a-model-without-data/index.html",
    "title": "Can you have a model without data?",
    "section": "",
    "text": "In frequentist statistics, the paradigm in which much of statistical practice is done, has a specific requirement: we need data before we can attribute estimates to (i.e., “fit”) our model. Yes, we might pre-specify it’s form, and be quite confident in what that looks like, but ultimately before we get an answer, we need the data.\nFor example, suppose we are interested in the proportion of individuals with heart disease. We can specify an assumed model:\n\\[X \\sim Bernoulli(p)\\]\nwhere \\(X \\in \\{0,1\\}\\) and \\(p \\in [0,1]\\).\nThat is, we assume that whether an individual has heart disease is a coin flip with probability \\(p\\). Our goal is to estimate what \\(p\\) is.\nWe plan to use the typical approach for estimating a population proportion such that:\n\\[\\hat{p} = \\frac{\\sum_{i=1}^nx_i}{n} \\hskip.2in Var(\\hat{p}) = \\frac{\\hat{p}(1-\\hat{p})}{n}\\]\nwhere \\(x_i\\) is the indicator of whether or not individual \\(i\\) in the sample has heart disease, and \\(n\\) is the total number of individuals in the sample. That is, we take the average, or sample proportion. The variance provides a window of uncertainty in our estimate.\nOkay let’s do it.\nBut wait, in order for us to get a numerical quantity to work with, we need data to plug into these equations. That is the point. Our model in this paradigm becomes data focused, such that a sample is required. And a large enough one at that.\nOur model of the world is completely dependent on collecting and entering a sample into the estimators, despite what we may already know about heart disease rates. Thus, it is only informed by the data at hand.\nOkay, fine. So we find a dataset related to heart disease:\nCode\ndat &lt;- cheese::heart_disease\ndat\n\n\n# A tibble: 303 × 9\n     Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n 1    63 Male   Typical angina     145         233 TRUE             150\n 2    67 Male   Asymptomatic       160         286 FALSE            108\n 3    67 Male   Asymptomatic       120         229 FALSE            129\n 4    37 Male   Non-anginal pain   130         250 FALSE            187\n 5    41 Female Atypical angina    130         204 FALSE            172\n 6    56 Male   Atypical angina    120         236 FALSE            178\n 7    62 Female Asymptomatic       140         268 FALSE            160\n 8    57 Female Asymptomatic       120         354 FALSE            163\n 9    63 Male   Asymptomatic       130         254 FALSE            147\n10    53 Male   Asymptomatic       140         203 TRUE             155\n# ℹ 293 more rows\n# ℹ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\nAnd we plug our estimates into the formulas and get our result:\nCode\n# Sample proportion\np_hat &lt;- mean(dat$HeartDisease == \"Yes\")\n\n# Sample size\nn &lt;- nrow(dat)\n\n# Standard error\nse &lt;- sqrt((p_hat * (1- p_hat)) / n)\n\n# Construct confidence interval\ndata.frame(\n  Estimate = paste0(round(p_hat * 100, 2), \"%\"),\n  Lower = paste0(round(100 * (p_hat - qnorm(.96) * se), 2), \"%\"),\n  Upper = paste0(round(100 * (p_hat + qnorm(.96) * se), 2), \"%\")\n) |&gt;\n  knitr::kable(format = \"html\") |&gt;\n  kableExtra::kable_styling(full_width = FALSE) |&gt;\n  kableExtra::add_header_above(c(\"\", \"92% Confidence Interval\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n92% Confidence Interval\n\n\n\nEstimate\nLower\nUpper\n\n\n\n\n45.87%\n40.86%\n50.89%\nDespite what we may think of this result (which is certainly high in any general population context), there’s not much wiggle room with respect to the output. The data is what it is: we estimate that \\(p\\) is somewhere between 41% and 51% with 92% confidence. And that’s it.\nWhat if this sample wasn’t able to be gathered all at once? What if we already knew stuff about the rate of heart disease? What if we wanted our estimates to be informed by prior information we had?\nWhen we consider our data being sequentially collected, we run into problems early on.\nCode\n# Set grid\nn_obs &lt;- c(1, 5, 10, 25, 50, 100, 200, nrow(dat))\n\n# Set values\np_hat &lt;- c()\nn &lt;- c()\nse &lt;- c()\n\n# Iterate the grid\nfor(i in 1:length(n_obs)) {\n  \n  # Compute estimates\n  temp_n &lt;- n_obs[i]\n  temp_p_hat &lt;- mean(dat$HeartDisease[1:temp_n] == \"Yes\")\n  temp_se &lt;- sqrt((temp_p_hat * (1 - temp_p_hat)) / temp_n)\n  \n  # Add to lists\n  p_hat &lt;- c(p_hat, temp_p_hat)\n  n &lt;- c(n, temp_n)\n  se &lt;- c(se, temp_se)\n  \n}\n\n# Make a plot\nlibrary(ggplot2)\ndata.frame(\n  p_hat = c(NA, p_hat), \n  n = c(0, n),\n  se = c(NA, se)\n) |&gt;\n  ggplot() + \n  geom_point(\n    aes(\n      x = factor(n),\n      y = p_hat\n    ),\n    size = 3\n  ) +\n  geom_linerange(\n    aes(\n      x = factor(n),\n      ymin = p_hat - qnorm(.96) * se,\n      ymax = p_hat + qnorm(.96) * se\n    ),\n    linewidth = 1\n  ) +\n  coord_flip() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray\"),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14)\n  ) +\n  scale_y_continuous(name = \"Estimated p (92% CI)\", labels = scales::percent) +\n  xlab(\"Sample size\")\nFirst, when we have no data, we can’t get any estimate (obviously). We have to pretend we know nothing about what the rate of heart disease is. After adding only 1 observation, our estimate for \\(p\\) is 0% with a 92% confidence interval ranging from 0% to 0%. This is not useful or informative, as that estimate was based on a single individual. Then, when we just add 4 more observations, our estimate of \\(p\\) becomes wildly uncertain (40% with a 92% confidence interval from 2% to 78%). This accumulated information is inconsistent and counter-intuitive (of course I’m using large sample methods, so we could use more appropriate small sample approaches, but that’s part of the point). Eventually as more data is added, the estimate gets more precise, but, again, completely driven by the data.\nThe bottom line being that in the frequentist paradigm, we are handcuffed. We can’t mathematically provide estimates until there is sufficient data collected, despite what our intuition or prior knowledge tells us about the parameter of interest beforehand.\nWhat if we have no data, or very little? What if we need to make decisions along the way before all of the data is collected, using our best estimate as of now? As we saw above, we need to wait to have sufficient data to get something reliable.\nIs there a way to provide a starting point about what we think the true rate of heart disease is, and then have our estimates be informed or augmented by evidence?\nYes, by being a Bayesian."
  },
  {
    "objectID": "posts/can-you-have-a-model-without-data/index.html#prior",
    "href": "posts/can-you-have-a-model-without-data/index.html#prior",
    "title": "Can you have a model without data?",
    "section": "Specify a prior distribution",
    "text": "Specify a prior distribution\nBefore any of the data is collected, we can use our subject matter knowledge about a phenomenon as to where we think a parameter value lies.\nIn the example above, suppose we thought it’s likely that the true parameter value \\(p\\) is somewhere around 35% in this population, of course allowing for some uncertainty.\nWe can assign a prior distribution to \\(p\\) using the Beta distribution (you could use anything you wanted that adheres to your prior knowledge, it just happens that this distribution works nicely in the case of proportions, an example of the conjugate prior):\n\\[p \\sim Beta(\\alpha = 4.5, \\beta = 7.5)\\]\nwhere the probability density function (PDF) is defined as:\n\\[f(x|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\]\nwhere \\(x \\in [0,1]\\), \\(\\alpha, \\beta &gt; 0\\), and \\(\\Gamma(n) = (n-1)!\\).\nAnd we can construct a plot to visualize our initial beliefs about the parameter \\(p\\):\n\n\nCode\n# Get the density values\nx &lt;- seq(0, 1, .01)\ny &lt;- dbeta(x, 4.5, 7.5)\n\n# Make the plot\ndata.frame(x, y) |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = x,\n      y = y\n    )\n  ) +\n  geom_vline(\n    xintercept = c(.2, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nSo initially we think there is a 72% chance that the true value of \\(p\\) is between 20% and 50% (taken as the area under the curve between those two points), with more probability mass towards the center.\nIn this sense, we have our model estimate already in its complete form. If the current state of information isn’t sufficient, then we can collect data to help guide/inform our prior belief. So instead of requiring a (large enough) sample to realize any numerical estimate, we have one with zero data points. As we add data, our model will update proportionally/appropriately to the amount of new information it contains. Therefore, we can think of this prior distribution as equivalent to our current posterior distribution, whether we got here from prior data, intuition, or a plain guess, it doesn’t really matter. Our current knowledge of \\(p\\) captures all that we know about it, and will only change as new information is added. Thus, we have constructed a model with no data."
  },
  {
    "objectID": "posts/can-you-have-a-model-without-data/index.html#updating-the-model-with-data",
    "href": "posts/can-you-have-a-model-without-data/index.html#updating-the-model-with-data",
    "title": "Can you have a model without data?",
    "section": "Updating the model with data",
    "text": "Updating the model with data\nNow we want a way to update our prior (or current) knowledge of the parameter of interest as new information comes in. The result of this is called the posterior distribution, which tells us where the parameter value(s) are most likely to be, given our prior beliefs + new data. The derivation of this distribution is done through Bayes’ theorem.\nIn our example (and any analysis), the posterior distribution is written as:\n\\[P(p|data) = \\frac{P(p)P(data|p)}{P(data)}\\]\nThe \\(P(p)\\) is the prior distribution, which we saw above, \\(P(data|p)\\) is the likelihood of observing our data given a particular value of \\(p\\), and \\(P(data)\\) is the probability of observing our dataset across all values of \\(p\\) (i.e., the law of total probability). In general, the denominator is not dependent on the parameter, and since we’re conditioned on the \\(data\\), this just amounts to a normalizing constant to ensure the posterior distribution is a valid probability distribution. Thus, we only need to concern ourselves with the form of the numerator, and can write the posterior as proportional to the product of the prior and likelihood:\n\\[P(p|data) \\propto P(p)P(data|p)\\]\n\nDefine the likelihood\nSimilar to what we saw in the frequentist approach above, the likelihood of the data in our Bayesian model can be thought of as a Bernoulli random variable, where each patient has heart disease or they don’t, for a given probability \\(p\\). Because our observations are independent, the collection of these “coin flips” can be summarized using the Binomial distribution:\n\\[H|n, p \\sim Binomial(n,p)\\]\nwhere \\(n\\) is the sample size, \\(p\\) is the probability of heart disease (the parameter of concern), and \\(H\\) is the total number of patients with heart disease in a sample. The probability mass function (PMF) for this distribution looks like:\n\\[P(data|p) = P(H|n,p) = \\frac{n!}{H!(n-H)!}p^H(1-p)^{n-H}\\]\nSo for a given sample size and probability, we can compute the likelihood of observing any number of patients with heart disease.\n\n\nDerive the posterior\nAs mentioned, the posterior is derived by taking the prior distribution multiplied by the likelihood function.\n\\[\n\\begin{equation}\n\\begin{split}\nP(p|data) & \\propto P(p)P(data|p) \\\\\n& = P(p)P(H|n, p)\\\\\n& = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1} \\frac{n!}{H!(n-H)!}p^H(1-p)^{n-H} \\\\\n& \\propto p^{\\alpha - 1 + H}(1-p)^{\\beta - 1 + n - H}\n\\end{split}\n\\end{equation}\n\\]\nIt turns out this is just another Beta distribution with a different parameterization (note the * to differentiate from the prior parameter values):\n\\[p|N,Y \\sim Beta(\\alpha^* = \\alpha + H, \\beta^* = \\beta + n - H)\\]\nNotice that we only needed the kernel to classify this distribution, because that part was dependent on the parameter \\(p\\). The rest is just a constant that normalizes it to be a valid probability distribution (as mentioned earlier), meaning it sums (integrates) to 1. Thus, since we know it’s a Beta, we can write out the full posterior PDF:\n\\[\n\\begin{equation}\n\\begin{split}\nf(p|\\alpha^*, \\beta^*) & = \\frac{\\Gamma(\\alpha^* + \\beta^*)}{\\Gamma(\\alpha^*)\\Gamma(\\beta^*)}p^{\\alpha^*-1}(1-p)^{\\beta^*-1} \\\\\n& = \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + H)\\Gamma(\\beta + n - H)}p^{\\alpha + H-1}(1-p)^{\\beta + n - H-1} \\\\\n\\end{split}\n\\end{equation}\n\\]\nWhat this tells us is that from our initial model, the posterior distribution is just moved/shifted as new data comes in. Also notice the effect of sample size as clearly indicated by the equation: as more data comes in (i.e., higher \\(H\\) and \\(N\\) values), the more the prior distribution will be drowned out. Meaning that we are only straying away from the initial/prior belief “proportional” to how much new information is coming in. This is what allows us to have perfectly valid models and estimates, even with a single observation or no data at all.\n\n\nPlug in the data\nThe hard part is done. Now all we need to do is plug in our data into the posterior distribution. In our sample of 303 patients (\\(n\\)), we observed 139 patients with heart disease (\\(H\\)). Plotting it across the range of possible values for \\(p\\) looks like this:\n\n\nCode\n# Set sample stats\nH &lt;- sum(dat$HeartDisease == \"Yes\")\nn &lt;- nrow(dat)\n\n# Get the density values\nx &lt;- seq(0, 1, .01)\ny &lt;- dbeta(x, 4.5 + H, 7.5 + n - H)\n\n# Make the plot\ndata.frame(x, y) |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = x,\n      y = y\n    )\n  ) +\n  geom_vline(\n    xintercept = c(.4, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nAfter our update using the data set (appended to our prior belief), we now estimate there is a 94% chance that the true value of \\(p\\) is between 40% and 50% (taken as the area under the curve between those two points), again with more probability mass towards the center.\n\n\nIncremental updates\nTo drive the point home, we’ll now revisit how our estimates change when data is added sequentially, and contrast that with what we saw from the frequentist approach above. To do this, we can just evaluate the posterior distribution at incremental chunks of our dataset to see how it changes as more data is added (assuming some sort of chronological structure to the data).\n\n\nCode\n# Set values\np &lt;- seq(0, 1, .01)\nresults &lt;- data.frame(n = 0, p = p, y = dbeta(p, 4.5, 7.5))\n\n# Iterate the grid\nfor(i in 1:length(n_obs)) {\n  \n  # Compute estimates\n  temp_n &lt;- n_obs[i]\n  temp_H&lt;- sum(dat$HeartDisease[1:temp_n] == \"Yes\")\n  temp_y &lt;- dbeta(p, 4.5 + temp_H, 7.5 + temp_n - temp_H)\n  \n  # Store values\n  results &lt;-\n    results |&gt;\n    rbind(\n      data.frame(\n        n = temp_n,\n        p = p,\n        y = temp_y\n      )\n    )\n  \n}\n\n# Make the plot\nresults |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = p,\n      y = y\n    )\n  ) + \n  facet_wrap(~paste0(\"n = \", n) |&gt; forcats::fct_reorder(y, max)) +\n  geom_vline(\n    xintercept = c(.4, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nAs done before, the red lines indicate the amount of the posterior distribution between 40% and 50%.\nFirst, we actually have an estimate before there is any data collected (when \\(n=0\\)). As we add a few observations, it only changes a little, but our estimates still retain our prior information about \\(p\\). Then as more data is collected, we see the posterior distribution become much more precise in where it estimates \\(p\\) to be.\nThis smooth integration of, and transition from, the prior knowledge we incorporate into the model to the information augmented by the data we collect is one reason why I think Bayesian thinking is better suited for scientific modeling. A more natural accumulation of knowledge, erasing the boundaries between what we already know (which should be considered a form of “data” itself) and hard data collected on a spreadsheet. It changes the way you approach the problem: instead of focusing right away on the data, which will be exhausted once it’s used, you focus on conceptualizing the living, breathing model of the world that generated it, and thus allow data to only contribute to that model as seen fit."
  },
  {
    "objectID": "posts/ways-to-ensure-success-of-statistical-project/index.html",
    "href": "posts/ways-to-ensure-success-of-statistical-project/index.html",
    "title": "5 ways to help ensure success of a statistical project",
    "section": "",
    "text": "Sometimes stats projects don’t go as planned. There are delays, setbacks, surprises, ambiguity, scope creep…the list goes on. All of these things can lead to a seemingly longer list of questions than what was started with: Did we answer the research question? Are we confident in the result? What do we do now? It’s a sense of dissatisfaction.\nMany of these issues stem from the early phases of the project, and (maybe I should collect data on this) they can largely be alleviated when more care is taken at that stage. Actually, the book I’m currently reading summed it up perfectly, “Well-designed research is research capable of answering the question it’s trying to answer”. Sounds obtuse, but it is undoubtedly true, even for projects outside of what you may define as “research”. This is about proper planning. So here are 5 things that can help increase the likelihood of a successful statistical project:\n\n1. Assemble your team…early\nIt’s often a misconception that the statistician’s role is to simply analyze, or “run the tests on”, the data at the end once it is collected. This is far from optimal. Statistical analysis is not systematic or mechanical. Rather, it requires knowledge and intuition about the subject matter context. Add in a lack of transparency to the data collection itself, the chances of lost insight definitely increase. In fact, it might be the case that a few poor design choices end up adding huge complexities in answering the original question, or maybe even make it impossible altogether. So, if you have a project idea, consult your statistician! Early and often during the development phase.\nNow, data people are certainly not the only ones you need involved early. Far from it. Who better to help shape the final product than the end-users—the people who will actually be using the information and know what works? If you want to integrate models into your operational workflow, what sort of resource or technical constraints may there be? Well, we probably have to talk to systems and IT people who will also likely need to commit their own resources for upkeep. And it comes full circle, because all of these nuances may even affect the statistical choices made from a mathematical perspective (i.e., design, modeling framework, etc.). There are plenty more important roles that could be described here, but the bottom line is that cross-functional collaboration, from the beginning, is crucial.\n\n\n2. Make the goals clear, then plan accordingly\nThere is a common issue in data projects of ambiguous or non-specific objectives. Statistics is inherently “gray”, by definition, because there is no right answer. Uncertainty always exists. So unless you already know exactly what you’re looking for in the data, without a clearly defined goal, you can find yourself spinning in circles and never know when what you’ve done is sufficient to move on.\nMy recommendation is to have multiple levels: the statistical goals and the real-world goals that they are supporting. The statistical goals should be stated as clear, specific questions with quantitative answers that data (among other things) will be used to estimate. However, we have to think about how these statistical quantities will be used afterwards. For example, the approach taken to estimate the likelihood of a customer buying a product (a statistical goal) may differ if we’re trying to decrease costs versus increase retention, especially when thinking about the solution as a whole. The question comes down to what we are trying to accomplish with the new information. Once that is clear, we can envision the roadmap for how it will be used, which can be an anchor for developing the right methodology, making analytic decisions easier to manage.\n\n\n3. Think about taking action\nOnce you obtain the new information you set out to find, what are you going to do about it? Under what circumstances? Based on which results? Having some inkling as to what it is going to enable (or disable) someone to do–not just in general, but a specific example–adds clarity to the practical implications of investing the time and money into finding the answers. These things can unravel many of the nuances that were originally an oversight, and may end up causing changes to how the information gets disseminated, who gets involved and when, or even the math itself. All in all, it allows for more proper design at the beginning, a reduction of wasted time/resources, and a better chance of finding the right solution. The reason we perform statistical analysis is (or should be) to inform some action or decision. If it doesn’t, then you may need to think about why that is and adjust. My favorite way to frame this to someone is by asking a simple question: “If you knew X at time Y then you could do Z. What are X, Y and Z?”.\n\n\n4. Create a tangible product\nIt may sound trivial, but it’s worth thinking about (and even explicitly defining). By what means will the result or solution be delivered? To whom? When? How? Is it going to be a comprehensive report or just a number sent in an email? It might even be a model deployed in the organization’s systems and workflows, or an application hosted on the web. These tell you all sorts of things about who should get involved (see #1) and what it will take to get there. It is about ensuring that the right information gets to the right people at the right time in an expected and predictable manner. It can be the case that a data project fails not because of bad statistics or models, but because it wasn’t disseminated optimally. Having a tangible end-product that you can work towards keeps everyone’s eye on the ball, exposes where the problems are, helps you plan deliverables, create milestones, and makes it clear when you are veering off course.\n\n\n5. Answer the question, “did it work?”\nOftentimes when you look at a statistic like a p-value, it leaves an empty feeling like you haven’t been convinced. That’s because it does not yet directly translate to the real-world impact the new insight is supposed to address. Maybe it’s useful during data analysis, but we should be thinking beyond the inferences made from the sample at hand to what we need to see to convince us that the results really matter. If the information we’ve garnered is actually useful, then we should expect improvements to play out where the rubber meets the road once it is utilized. The most direct way to do this: test it."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "",
    "text": "This dataset comes from the 2011 Annual Resident Survey in Durham, NC.\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Import the dataset\ndat &lt;- read_delim(\n  file = \"https://query.data.world/s/zr3uaxpaagbzddttoreosktj2zy7lm?dws=00000\", \n  delim = \";\",\n  na = c(\"\", \" \", \"NA\", \"N/A\")\n) |&gt;\n  \n  # Keep a few columns\n  transmute(\n    QOL = \n      q3f_quality_of_life_in_city |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Very Dissatisfied\",\n        \"Dissatisfied\",\n        \"Neutral\",\n        \"Satisfied\",\n        \"Very Satisfied\"\n      ),\n    Age = str_remove(`18_34_years`, \"(?i)\\\\syears$\") |&gt; factor(),\n    Income = \n      q38_annual_household_income |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Under $30,000\",\n        \"$30,000 to $59,999\",\n        \"$60,000 to $99,999\",\n        \"$100,000 or more\"\n      ),\n    Sex = q34_respondents_gender\n  ) |&gt;\n  \n  # Remove missing cases\n  na.omit()\nSuppose we are interested in understanding the relationship between resident age and their perceived quality of life in the city, after adjusting for gender and annual household income. We have the following observed distribution (note that we’ve removed missing data for simplicity):\nCode\ndat |&gt; \n  \n  # Compute group summaries\n  summarize(\n    N = n(),\n    .by = \n      c(\n        QOL,\n        Age\n      )\n  ) |&gt;  \n  \n  # Flip the order\n  mutate(\n    QOL = fct_rev(QOL)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() + \n  geom_col(\n    aes(\n      x = Age,\n      y = N,\n      fill = QOL\n    ),\n    color = \"black\",\n    alpha = .75\n  ) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(hjust = .5)\n  ) +\n  xlab(\"Respondent Age (years)\") +\n  ylab(\"Count\") +\n  labs(title = \"Quality of life in the city\")\nOverall, it looks like older respondents tend to report more pessimistic views of quality of life."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "What does that mean?",
    "text": "What does that mean?\nLet’s clarify this by using our model output directly. We’ll fit the model, adjusted for annual household income and gender, using the MASS::polr function. The age group 18-34 will serve as the reference category in which all other age groups will be compared against. Note: By default the package computes odds ratios in the opposite direction to what we what we want, so we invert them.\n\n\nCode\n# Fit the model\nmod &lt;-\n  MASS::polr(\n    formula = QOL ~ Age + Income + Sex,\n    data = dat,\n    Hess = TRUE\n  )\n\n# Make a table of odds-ratios\nmod$coefficients |&gt;\n  \n  # Convert to data frame\n  enframe(\n    name = \"Term\",\n    value = \"Estimate\"\n  ) |&gt;\n  \n  # Join to get the CI\n  inner_join(\n    y = \n      # Get the 95% confidence intervals\n      confint(mod) |&gt; \n      \n      # Convert to tibble, add the coefficient names\n      as_tibble() |&gt; \n      add_column(Term = names(mod$coefficients)),\n    by = \"Term\"\n  ) |&gt; \n  \n  # Filter to age factor only\n  filter(str_detect(Term, \"^Age\")) |&gt;\n  \n  # Clean up\n  mutate(\n    Term = str_remove(Term, \"^Age\"),\n    across(\n      where(is.numeric),\n      \\(x) sprintf(\"%.2f\", 1 / exp(x))\n    )\n  ) |&gt; \n  \n  # Rename\n  rename(\n    `Age (years)` = Term,\n    `Odds-ratio` = Estimate,\n    Lower = `97.5 %`,\n    Upper = `2.5 %`\n  ) |&gt;\n  \n  # Change location\n  relocate(Lower, .before = Upper) |&gt;\n  \n  # Add the reference row\n  add_row(\n    `Age (years)` = \"18-34\",\n    `Odds-ratio` = \"-\",\n    Lower = \"-\",\n    Upper = \"-\",\n    .before = 1\n  ) |&gt;\n  \n  # Make a table\n  knitr::kable(format = \"html\") |&gt;\n  kableExtra::kable_styling() |&gt;\n  kableExtra::add_header_above(c(\"\", \"\", \"95% CI\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% CI\n\n\n\nAge (years)\nOdds-ratio\nLower\nUpper\n\n\n\n\n18-34\n-\n-\n-\n\n\n35-44\n1.24\n0.67\n2.30\n\n\n45-54\n1.13\n0.64\n1.98\n\n\n55-64\n1.74\n0.97\n3.12\n\n\n65-74\n3.02\n1.40\n6.54\n\n\n75+\n0.93\n0.30\n2.88\n\n\n\n\n\n\n\nGenerally, the estimates pan out roughly how we suspected. In particular, the estimated odds of worse perceived quality of life in the city for 65-74 year olds are 3 times that of 18-34 year olds, after adjusting for annual household income and gender (with a 95% confidence interval of 1.4 to 6.5).\nAgain, this interpretation is assumed to hold true if “worse” is defined as very dissatisfied versus everything else, or very dissatisfied through satisfied versus very satisfied, and everything in between."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Continue…",
    "text": "Continue…\nWe said earlier that the model assumes the same odds ratios for any mutually exclusive comparison of the ordered response categories. Thus, we can free up this constraint by thinking of constructing a collection of binary logistic regression models: one for each of those ordinal comparisons. Specifically,\n\nVery Dissatisfied versus everything else\nVery Dissatisfied or Dissatisfied versus everything else\nVery Dissatisfied, Dissatisfied, or Neutral versus Satisfied or Very Satisfied\nVery Dissatisfied through Satisfied versus Very Satisfied\n\nThen, we simply just look to see if the resulting odds ratios are reasonably similar across all of those models. If so, then we can be somewhat confident that it can be reduced to a single model, and stick with our original proportional-odds estimates.\nMy preference is to do this in a plot."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Making the plot",
    "text": "Making the plot\nWe’ll cycle through the response categories, iteratively define the binary outcomes as described above, and then fit a logistic regression model for each definition. Once we do this, we get the following plot:\n\n\nCode\n# Set the number of comparisons\nn_comp &lt;- n_distinct(dat$QOL) - 1\n\n# Make each data set\n1:n_comp |&gt;\n  \n  # For each set\n  map_df(\n    function(.index) {\n      \n      # Extract the current response set\n      temp_resp &lt;- levels(dat$QOL)[1:.index]\n      \n      # Create binary outcome in the data\n      temp_dat &lt;- \n        dat |&gt;\n        \n        # Create target\n        mutate(\n          Response = \n            case_when(\n              QOL %in% temp_resp ~ 1,\n              TRUE ~ 0\n            )\n        )\n      \n      # Fit the binary logistic regression model\n      temp_mod &lt;-\n        glm(\n          formula = Response ~ Age + Income + Sex,\n          data = temp_dat,\n          family = \"binomial\"\n        )\n      \n      # Make a table of odds-ratios\n      temp_mod$coefficients |&gt;\n        \n        # Convert to data frame\n        enframe(\n          name = \"Term\",\n          value = \"Estimate\"\n        ) |&gt;\n        \n        # Join to get the CI\n        inner_join(\n          y = \n            # Get the 95% confidence intervals\n            confint.default(temp_mod) |&gt; \n            \n            # Convert to tibble, add the coefficient names\n            as_tibble() |&gt; \n            add_column(Term = names(temp_mod$coefficients)),\n          by = \"Term\"\n        ) |&gt; \n        \n        # Filter to age factor only\n        filter(str_detect(Term, \"^Age\")) |&gt;\n        \n        # Clean up\n        mutate(\n          Term = str_remove(Term, \"^Age\"),\n          across(\n            where(is.numeric),\n            \\(x) exp(x)\n          )\n        ) |&gt;\n        \n        # Rename\n        rename(\n          Age = Term,\n          OR = Estimate,\n          Lower = `2.5 %`,\n          Upper = `97.5 %`\n        ) |&gt;\n        \n        # Add the reference row\n        add_row(\n          Age = \"18-34\",\n          OR = 1,\n          Lower = 1,\n          Upper = 1,\n          .before = 1\n        ) |&gt;\n        \n        # Attach outcome level\n        add_column(QOL = levels(dat$QOL)[.index])\n      \n    },\n    .id = \"Order\"\n  ) |&gt;\n  \n  # Make the factor\n  mutate(\n    Order = as.numeric(Order),\n    QOL = factor(QOL) |&gt; fct_reorder(Order)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = QOL,\n      y = OR,\n      group = 1\n    )\n  ) +\n  geom_line(\n    aes(\n      color = Age\n    ),\n    linewidth = 1.5\n  ) +\n  geom_point(\n    aes(\n      color = Age\n    ),\n    size = 3\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = Lower,\n      ymax = Upper,\n      fill = Age\n    ),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 1, color = \"gray\") +\n  facet_wrap(~paste0(Age, \" years\")) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"none\",\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, size = 10),\n    plot.title = element_text(hjust = .5),\n    strip.text = element_text(size = 14)\n  ) +\n  xlab(\"Response\") +\n  ylab(\"Odds-ratio (95% CI) for being at or below response level\")\n\n\n\n\n\n\n\n\n\nUnfortunately we’re quite plagued by variability here, especially in the lower-end (i.e., very dissatisfied versus everything else), due to scanty event volumes, but you get the picture. Actually, for 65-74 year olds, the proportional-odds assumption seems to be a reasonable one: it was estimated earlier at 3.02, and we see the point estimates across these binary models vary between 2.5-3.5.\nFor other age categories, it may not be so good of an assumption. It looks like 35-44 and 55-64 year olds tend to have a much higher odds of responding very dissatisfied relative to 18-34 year olds, but there is much less of a difference (in all age categories) for the odds of responding very satisfied, suggesting something like older residents may make a point to select the least favorable response but don’t see much difference between being satisfied or very satisfied."
  },
  {
    "objectID": "posts/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "href": "posts/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "So what do we do in practice?",
    "text": "So what do we do in practice?\nFirst, the same proportional-odds assumptions hold for all covariates in the model, so we would also want to assess this for annual household income and gender. Second, if the assumption is not met, then we need to accommodate that by introducing more flexibility into the model. That may be by being clever with interaction terms, defining sensible groups to create, or by using separate binary models for each possible comparison, as we’ve done here. It’s really a judgement call."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CentralStatz Statistical & Data Sciences LLC",
    "section": "",
    "text": "Statistical and data science consulting in Central Wisconsin\n\n\n\n\n\n\n\n\nLoading…"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 5, 2024\n\n\nYour organization should be using R Shiny\n\n\nAlex Zajichek\n\n\n\n\n\nNo matching items"
  }
]